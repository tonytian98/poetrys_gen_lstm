{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "poetry_generation_LSTM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import numpy as np\n",
        "import string"
      ],
      "metadata": {
        "id": "60tsvx3m60SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(s):\n",
        "  return s.translate(str.maketrans('', '', string.punctuation))"
      ],
      "metadata": {
        "id": "sz5Fkonw8tFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4vZY1ARYQ7E"
      },
      "outputs": [],
      "source": [
        "def get_robert_frost():\n",
        "    word2idx = {'START': 0, 'END': 1}\n",
        "    current_idx = 2\n",
        "    sentences = []\n",
        "    max_length = 0\n",
        "    for line in open('drive/MyDrive/robert_frost.txt',encoding='utf-8'):\n",
        "        line = line.strip()\n",
        "        if line:\n",
        "            tokens = remove_punctuation(line.lower()).split()\n",
        "            sentence = []\n",
        "            for t in tokens:\n",
        "                if t not in word2idx:\n",
        "                    word2idx[t] = current_idx\n",
        "                    current_idx += 1\n",
        "                idx = word2idx[t]\n",
        "                sentence.append(idx)\n",
        "            sentence.append(1) # 1 meaning END\n",
        "            max_length = max(max_length, len(sentence))\n",
        "            sentences.append(sentence)\n",
        "    return sentences, word2idx, max_length\n",
        "\n",
        "\n",
        "def sentence2lists(sentence):\n",
        "  l=[]\n",
        "  for i in range(len(sentence)):\n",
        "    l.append(sentence[:i+1])\n",
        "  return l\n",
        "\n",
        "def padding(sentence, max_length):\n",
        "  return [0 for i in range(max_length- len(sentence))]+sentence\n",
        "\n",
        "def final_data(sentences, max_l):\n",
        "  l= []\n",
        "  for s in sentences:\n",
        "    l += sentence2lists(s)\n",
        "  for i in range(len(l)):\n",
        "    l[i] = padding(l[i], max_l)\n",
        "    \n",
        "  return np.array(l)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(model, pi, word2idx,n):\n",
        "        # convert word2idx -> idx2word\n",
        "  idx2word = {v:k for k,v in word2idx.items()}\n",
        "  V = len(pi)\n",
        "\n",
        "  n_lines = 0\n",
        "\n",
        "  X = [0 for i in range(max_length-2) ]+[np.random.choice(V, p=pi) ]\n",
        "  print(idx2word[X[-1]], end=\" \")\n",
        "\n",
        "  while n_lines < n:\n",
        "    xx= np.array(X).reshape(1,-1)\n",
        "    probs = model.predict(xx).reshape(-1,)\n",
        "    word_idx = np.random.choice(V, p=probs)\n",
        "    X.append(word_idx)\n",
        "    X= X[1:]\n",
        "    if word_idx > 1:\n",
        "      # it's a real word, not start/end token\n",
        "      word = idx2word[word_idx]\n",
        "      print(word, end=\" \")\n",
        "    elif word_idx == 1:\n",
        "      # end token\n",
        "      n_lines += 1\n",
        "      print('')\n",
        "      if n_lines < n:\n",
        "        X =[ 0 for i in range(max_length-2) ]+[ np.random.choice(V, p=pi) ]# reset to start of line\n",
        "        print(idx2word[X[-1]], end=\" \")\n",
        "def generate_poetry(model,n):\n",
        "\n",
        "    # determine initial state distribution for starting sentences\n",
        "  V = len(word2idx)\n",
        "  pi = np.zeros(V)\n",
        "  for sentence in sentences:\n",
        "    pi[sentence[0]] += 1\n",
        "  pi /= pi.sum()\n",
        "\n",
        "  generate(model,pi, word2idx,n)"
      ],
      "metadata": {
        "id": "gb08waawq_iJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences, word2idx, max_length = get_robert_frost()\n",
        "l = final_data(sentences,max_length)\n",
        "N = len(sentences)\n",
        "D = 100\n",
        "V = len(word2idx)\n",
        "X = l[:,:-1]\n",
        "Y = tf.keras.utils.to_categorical( l[:,-1], num_classes = V)\n",
        "print(V)\n",
        "print(Y[1].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDqW69LmLlYr",
        "outputId": "272bab4c-6eaa-4804-8fcc-db006c2bb062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2199\n",
            "(2199,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(V, D, input_length = max_length-1))\n",
        "#model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(V, activation = 'softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer = tf.keras.optimizers.Adam(0.01), metrics = ['accuracy'])\n",
        "\n",
        "history = model.fit(X,Y,epochs = 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Dl6dNfH9tdW",
        "outputId": "1f3c4120-2314-4e4f-a7d4-6d3ea2cb1559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "380/380 [==============================] - 4s 7ms/step - loss: 5.9785 - accuracy: 0.1359\n",
            "Epoch 2/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 5.2687 - accuracy: 0.1578\n",
            "Epoch 3/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 4.7598 - accuracy: 0.1829\n",
            "Epoch 4/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 4.3174 - accuracy: 0.2103\n",
            "Epoch 5/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 3.8397 - accuracy: 0.2435\n",
            "Epoch 6/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 3.4695 - accuracy: 0.2882\n",
            "Epoch 7/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 3.1283 - accuracy: 0.3310\n",
            "Epoch 8/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 2.8736 - accuracy: 0.3663\n",
            "Epoch 9/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 2.6619 - accuracy: 0.4023\n",
            "Epoch 10/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 2.5327 - accuracy: 0.4219\n",
            "Epoch 11/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 2.4376 - accuracy: 0.4458\n",
            "Epoch 12/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 2.3325 - accuracy: 0.4617\n",
            "Epoch 13/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 2.2229 - accuracy: 0.4876\n",
            "Epoch 14/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 2.1408 - accuracy: 0.5000\n",
            "Epoch 15/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 2.0953 - accuracy: 0.5095\n",
            "Epoch 16/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 2.0628 - accuracy: 0.5197\n",
            "Epoch 17/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 2.0612 - accuracy: 0.5149\n",
            "Epoch 18/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 2.0374 - accuracy: 0.5256\n",
            "Epoch 19/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.9968 - accuracy: 0.5246\n",
            "Epoch 20/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.9370 - accuracy: 0.5437\n",
            "Epoch 21/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.9177 - accuracy: 0.5429\n",
            "Epoch 22/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.8873 - accuracy: 0.5525\n",
            "Epoch 23/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.8440 - accuracy: 0.5629\n",
            "Epoch 24/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.8476 - accuracy: 0.5591\n",
            "Epoch 25/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.8034 - accuracy: 0.5723\n",
            "Epoch 26/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7980 - accuracy: 0.5709\n",
            "Epoch 27/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.8109 - accuracy: 0.5674\n",
            "Epoch 28/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7261 - accuracy: 0.5895\n",
            "Epoch 29/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7190 - accuracy: 0.5910\n",
            "Epoch 30/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7497 - accuracy: 0.5841\n",
            "Epoch 31/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7573 - accuracy: 0.5787\n",
            "Epoch 32/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7118 - accuracy: 0.5893\n",
            "Epoch 33/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.9416 - accuracy: 0.5435\n",
            "Epoch 34/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 2.0991 - accuracy: 0.5010\n",
            "Epoch 35/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 2.0110 - accuracy: 0.5161\n",
            "Epoch 36/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.8619 - accuracy: 0.5509\n",
            "Epoch 37/100\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.8233 - accuracy: 0.5630\n",
            "Epoch 38/100\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.7639 - accuracy: 0.5728\n",
            "Epoch 39/100\n",
            "380/380 [==============================] - 4s 10ms/step - loss: 1.7115 - accuracy: 0.5876\n",
            "Epoch 40/100\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.6862 - accuracy: 0.5932\n",
            "Epoch 41/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6889 - accuracy: 0.5917\n",
            "Epoch 42/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6970 - accuracy: 0.5878\n",
            "Epoch 43/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7175 - accuracy: 0.5829\n",
            "Epoch 44/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.8049 - accuracy: 0.5668\n",
            "Epoch 45/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7689 - accuracy: 0.5732\n",
            "Epoch 46/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7807 - accuracy: 0.5704\n",
            "Epoch 47/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7826 - accuracy: 0.5744\n",
            "Epoch 48/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7721 - accuracy: 0.5755\n",
            "Epoch 49/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7460 - accuracy: 0.5789\n",
            "Epoch 50/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7079 - accuracy: 0.5923\n",
            "Epoch 51/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7006 - accuracy: 0.5918\n",
            "Epoch 52/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7471 - accuracy: 0.5897\n",
            "Epoch 53/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7559 - accuracy: 0.5770\n",
            "Epoch 54/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7472 - accuracy: 0.5852\n",
            "Epoch 55/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7295 - accuracy: 0.5883\n",
            "Epoch 56/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7254 - accuracy: 0.5849\n",
            "Epoch 57/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7035 - accuracy: 0.5912\n",
            "Epoch 58/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6724 - accuracy: 0.6000\n",
            "Epoch 59/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6648 - accuracy: 0.5983\n",
            "Epoch 60/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6470 - accuracy: 0.6039\n",
            "Epoch 61/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6413 - accuracy: 0.6037\n",
            "Epoch 62/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6784 - accuracy: 0.5983\n",
            "Epoch 63/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7009 - accuracy: 0.5927\n",
            "Epoch 64/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7159 - accuracy: 0.5889\n",
            "Epoch 65/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7169 - accuracy: 0.5890\n",
            "Epoch 66/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6814 - accuracy: 0.5926\n",
            "Epoch 67/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7022 - accuracy: 0.5886\n",
            "Epoch 68/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7541 - accuracy: 0.5834\n",
            "Epoch 69/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7043 - accuracy: 0.5948\n",
            "Epoch 70/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6666 - accuracy: 0.5978\n",
            "Epoch 71/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6438 - accuracy: 0.6059\n",
            "Epoch 72/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6520 - accuracy: 0.6061\n",
            "Epoch 73/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6285 - accuracy: 0.6084\n",
            "Epoch 74/100\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.6404 - accuracy: 0.6090\n",
            "Epoch 75/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6267 - accuracy: 0.6137\n",
            "Epoch 76/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6203 - accuracy: 0.6122\n",
            "Epoch 77/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6336 - accuracy: 0.6118\n",
            "Epoch 78/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6443 - accuracy: 0.6066\n",
            "Epoch 79/100\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.6760 - accuracy: 0.6051\n",
            "Epoch 80/100\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.6529 - accuracy: 0.6060\n",
            "Epoch 81/100\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.6844 - accuracy: 0.5968\n",
            "Epoch 82/100\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.6646 - accuracy: 0.6050\n",
            "Epoch 83/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6848 - accuracy: 0.6046\n",
            "Epoch 84/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7035 - accuracy: 0.5916\n",
            "Epoch 85/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6719 - accuracy: 0.5969\n",
            "Epoch 86/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6564 - accuracy: 0.6062\n",
            "Epoch 87/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6393 - accuracy: 0.6108\n",
            "Epoch 88/100\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.6049 - accuracy: 0.6118\n",
            "Epoch 89/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.5948 - accuracy: 0.6192\n",
            "Epoch 90/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.5795 - accuracy: 0.6257\n",
            "Epoch 91/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6057 - accuracy: 0.6136\n",
            "Epoch 92/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.7461 - accuracy: 0.5974\n",
            "Epoch 93/100\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.6914 - accuracy: 0.6018\n",
            "Epoch 94/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6379 - accuracy: 0.6097\n",
            "Epoch 95/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6281 - accuracy: 0.6151\n",
            "Epoch 96/100\n",
            "380/380 [==============================] - 3s 7ms/step - loss: 1.6605 - accuracy: 0.6072\n",
            "Epoch 97/100\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.7296 - accuracy: 0.5980\n",
            "Epoch 98/100\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.6743 - accuracy: 0.6112\n",
            "Epoch 99/100\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.6598 - accuracy: 0.6094\n",
            "Epoch 100/100\n",
            "380/380 [==============================] - 3s 8ms/step - loss: 1.6086 - accuracy: 0.6217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_poetry(model,10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU_7-lfOaIyJ",
        "outputId": "c87bdae3-bd21-4576-d9d0-99f2c1d6ade2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "with the brook going to rain \n",
            "that made him throw his bare legs out of the grange \n",
            "you want my creaking feet \n",
            "to the eye of i had halted more on the earth \n",
            "that overflowing like will leave it inward on her books \n",
            "its for fifty dollars for one langshang cock \n",
            "i turned and repented but coming back \n",
            "she hadnt found one leg and a crutch \n",
            "to carry again to you \n",
            "shes let me say the man was after \n"
          ]
        }
      ]
    }
  ]
}